---
title: "Twitter Edchat Inbteractions Project Analysis"
author: "K. Bret Staudt Willet"
date: "10/28/2019"
output: 
    html_document:
        toc: true
        float_toc: true
---

# Get set up

This section loads the data and packages and starts to process the data, but doesn't calculate any statistics or create any results.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
usethis::use_git_ignore(c("*.csv", "*.rds"))
```

## Load packages

```{r, include=FALSE}
library(tidyverse)
library(lubridate)
library(rtweet)
library(compute.es)
library(igraph)
```

## Load the data

Having completed the steps in the setup.Rmd file, you now have the dataset stored in your local repository and can load it as usual. This project uses Twitter #Edchat data that have been run through the `rtweet` R package, which queries the Twitter API to return the most complete set of tweet metadata available, while also removing deleted and protected tweets. See https://rtweet.info/ for details on `rtweet`.

```{r, include=FALSE}
edchat_og <- read.csv("edchat_rtweet_df.csv", 
                   header=TRUE, 
                   colClasses= c(status_id='character',
                                 reply_to_status_id='character',
                                 user_id='character',
                                 reply_to_user_id='character',
                                 text='character'
                                 )
                   ) %>%
    filter(protected==FALSE)
```

# Data analysis

## RQ1. What does participation in Twitter #Edchat look like?

*Time frame*

```{r, include=TRUE, echo=FALSE}
#OlsonNames()  ## returns full list of timezones
edchat <- edchat_og %>% 
    mutate(created_at = created_at %>% ymd_hms() %>% with_tz(tzone="US/Eastern"),
           day = weekdays(created_at),
           is_sync = ifelse(day == "Tuesday" &
                                hour(created_at) >= 18 & 
                                hour(created_at) < 21,
                            TRUE,
                            FALSE
                             )
           )
time_start <- min(edchat$created_at)
time_end <- max(edchat$created_at)
n_months <- (time_end - time_start) %>% time_length(unit="months")

paste("Tweets from", date(time_start), 
      "to", date(time_end), 
      "(", round(n_months, 2), "months )")
```

*Number of tweets and tweeters*

```{r, include=TRUE, echo=FALSE}
n_tweets <- edchat$status_id %>% unique() %>% length()
n_tweeters <- edchat$user_id %>% unique() %>% length()
paste("Number of unique tweets:", n_tweets); paste("Number of unique tweeters:", n_tweeters)
```

*Tweets per month per user*

```{r, include=TRUE, echo=FALSE}
time_start <- edchat$created_at %>% ymd_hms() %>% min()
time_end <- edchat$created_at %>% ymd_hms() %>% max()
n_months <- (time_end - time_start) %>% time_length(unit="months")

paste("Tweets per month per user:", round(n_tweets / n_months / n_tweeters, 2))
```

```{r, include=TRUE, echo=FALSE}
freq_tweeters <- edchat %>% 
    pull(user_id) %>% 
    table() %>% 
    as.data.frame() %>% 
    rename(user_id = ".",
           tweets_made_edchat = Freq) %>%
    mutate(tweets_per_month = round(tweets_made_edchat / n_months, 2)) %>%
    arrange(desc(tweets_per_month))

monthly_tweets_mean <- freq_tweeters$tweets_per_month %>% mean %>% round(2)
monthly_tweets_sd <- freq_tweeters$tweets_per_month %>% sd %>% round(2)
monthly_tweets_median <- freq_tweeters$tweets_per_month %>% median %>% round(2)
monthly_tweets_min <- freq_tweeters$tweets_per_month %>% min %>% round(2)
monthly_tweets_max <- freq_tweeters$tweets_per_month %>% max %>% round(2)

paste("Tweets per month per user:"); paste("Mean =", monthly_tweets_mean); paste("Standard Deviation =", monthly_tweets_sd); paste("Median =", monthly_tweets_median); paste("Range =", monthly_tweets_min, "to", monthly_tweets_max)
```

*Minimal participation.* Look at one-time tweeters.

```{r, include=TRUE, echo=FALSE}
one_timers <- freq_tweeters %>% filter(tweets_made_edchat==1)
paste("Tweeters who contributed only one tweet to #Edchat:", 
      nrow(one_timers),
      "(", round(100 * nrow(one_timers) / n_tweeters, 2), "% )"
      )
```

```{r, include=TRUE, echo=FALSE}
freq_tweeters_RT <- edchat %>% 
    filter(is_retweet) %>%
    pull(user_id) %>% 
    table() %>% 
    as.data.frame() %>% 
    rename(user_id = ".",
           retweets_made_edchat = Freq) %>%
    mutate(retweets_per_month = round(retweets_made_edchat / n_months, 2)) %>%
    arrange(desc(retweets_per_month))

RT_only <- freq_tweeters_RT %>% nrow()
paste("Tweeters who contributed by retweeting #Edchat tweets:", 
      RT_only,
      "(", round(100 * RT_only / n_tweeters, 2), "% )"
      )
```

```{r, include=TRUE, echo=FALSE}
one_timers_RT_only <- freq_tweeters_RT %>% filter(retweets_made_edchat==1)
paste("Tweeters who contributed only one retweet to #Edchat:", 
      nrow(one_timers_RT_only),
      "(", round(100 * nrow(one_timers_RT_only) / n_tweeters, 2), "% )"
      )
```

*Remove retweets*

```{r, include=TRUE, echo=FALSE}
edchat_no_rt <- edchat %>% filter(!is_retweet)

time_start_no_RT <- min(edchat_no_rt$created_at)
time_end_no_RT <- max(edchat_no_rt$created_at)
n_months_no_RT <- time_length(time_end_no_RT - time_start_no_RT, unit="months")

n_tweets_no_RT <- edchat_no_rt$status_id %>% unique() %>% length()
n_tweeters_no_RT <- edchat_no_rt$user_id %>% unique() %>% length()

paste("Number of unique tweets (no retweets):", n_tweets_no_RT); paste("Number of unique tweeters (no retweets):", n_tweeters_no_RT); paste("Tweets per month per user:", round(n_tweets_no_RT / n_months_no_RT / n_tweeters_no_RT, 2))
```

```{r, include=TRUE, echo=FALSE}
freq_tweeters_no_RT <- edchat_no_rt %>% 
    pull(user_id) %>% 
    table() %>% 
    as.data.frame() %>% 
    rename(user_id = ".",
           tweets_made_edchat = Freq) %>%
    mutate(tweets_per_month = round(tweets_made_edchat / n_months, 2)) %>%
    arrange(desc(tweets_per_month))

monthly_tweets_mean_no_RT <- freq_tweeters_no_RT$tweets_per_month %>% mean %>% round(2)
monthly_tweets_sd_no_RT <- freq_tweeters_no_RT$tweets_per_month %>% sd %>% round(2)
monthly_tweets_median_no_RT <- freq_tweeters_no_RT$tweets_per_month %>% median %>% round(2)
monthly_tweets_min_no_RT <- freq_tweeters_no_RT$tweets_per_month %>% min %>% round(2)
monthly_tweets_max_no_RT <- freq_tweeters_no_RT$tweets_per_month %>% max %>% round(2)

paste("Original tweets per month per user (no retweets):"); paste("Mean =", monthly_tweets_mean_no_RT); paste("Standard Deviation =", monthly_tweets_sd_no_RT); paste("Median =", monthly_tweets_median_no_RT); paste("Range =", monthly_tweets_min_no_RT, "to", monthly_tweets_max_no_RT)
```

*Minimal participation.* Look at one-time tweeters (no retweets).

```{r, include=TRUE, echo=FALSE}
one_timers_no_RT <- freq_tweeters_no_RT %>% filter(tweets_made_edchat==1)
paste("Tweeters who contributed only one tweet to #Edchat (no retweets):", 
      nrow(one_timers_no_RT),
      "(", round(100 * nrow(one_timers_no_RT) / n_tweeters_no_RT, 2), "% )"
      )
```

## RQ2. How are participants connected to each other in #Edchat? 

Continue looking at #Edchat tweet corpus minus retweets.

**RQ2a**: Replies 

First, calculate descriptive statistics for replies-per-tweeter.

```{r, include=FALSE}
freq_replies <- table(edchat_no_rt$reply_to_status_id) %>% 
    as.data.frame() %>% 
    rename(status_id = Var1,
           reply_count = Freq) %>%
    arrange(desc(reply_count))

n_reply_tweets <- sum(freq_replies$reply_count)
n_reply_tweeters <- nrow(freq_replies)
n_no_reply_no_RT_tweeters <- n_tweeters_no_RT - nrow(freq_replies)
p_reply_tweeters <- round(100 * n_reply_tweeters / n_tweeters_no_RT, 2)
p_reply_tweeters_of_all <- round(100 * n_reply_tweeters / n_tweeters, 2)

replies_mean <- freq_replies$reply_count %>% mean %>% round(2)
replies_sd <- freq_replies$reply_count %>% sd %>% round(2)
replies_median <- freq_replies$reply_count %>% median %>% round(2)
replies_min <- freq_replies$reply_count %>% min %>% round(2)
replies_max <- freq_replies$reply_count %>% max %>% round(2)

paste("Replies-per-tweeter statistics:"); paste(n_reply_tweeters, "(", p_reply_tweeters, "% of original tweeters and", p_reply_tweeters_of_all, "% of all tweeters) replied to someone."); paste("Mean =", replies_mean); paste("Standard Deviation =", replies_sd); paste("Median =", replies_median); paste("Range =", replies_min, "to", replies_max)
```

```{r, include=TRUE, echo=FALSE}
one_time_repliers <- freq_replies %>% filter(reply_count==1)
multi_repliers <- freq_replies %>% filter(reply_count>1)

n_multi_repliers <- nrow(multi_repliers)
p_multi_repliers <- round(100* n_multi_repliers / n_tweeters_no_RT, 2)
p_multi_repliers_of_all <- round(100* n_multi_repliers / n_tweeters, 2)

paste(n_multi_repliers, "(" , p_multi_repliers, "% of original tweeters and", p_multi_repliers_of_all, "% of all #Edchat tweeters) replied more than once.")
```

**RQ2b: Following/Followers Ratio**

```{r, include=FALSE}
hashtag_regex <- "#([0-9]|[a-zA-Z])+"
url_regex <- "http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+"

edchat_with_replies <- edchat_no_rt %>% 
    full_join(freq_replies, by='status_id') %>% 
    mutate(reply_count = ifelse(is.na(reply_count), 0, reply_count)) %>%
    filter(!is.na(user_id)) %>% 
    rename(profile_description = description) %>% 
    mutate(hashtag_count = str_count(text, hashtag_regex),
           hashtag_inclusion = ifelse(hashtag_count==0, 0, 1),
           url_count = str_count(text, url_regex),
           url_inclusion = ifelse(url_count==0, 0, 1),
           following_ratio = ifelse(followers_count==0,
                                    NA,
                                    friends_count / followers_count)
           )

# 25,638 tweeters in edchat_with_replies
# 95 of these have no one following them (edchat_with_replies$followers_count == 0)
# 631 of these are not following anyone (edchat_with_replies$friends_count == 0) 
```

Calculate descriptive statistics for Following/Followers Ratio.

```{r, include=FALSE}
following_ratios <- edchat_with_replies %>%
    distinct(user_id, .keep_all = TRUE) %>%
    pull(following_ratio)

following_ratio_mean <- following_ratios[which(!is.na(following_ratios))] %>% mean %>% round(2)
following_ratio_sd <- following_ratios[which(!is.na(following_ratios))] %>% sd %>% round(2)
following_ratio_median <- following_ratios[which(!is.na(following_ratios))] %>% median %>% round(2)
following_ratio_min <- following_ratios[which(!is.na(following_ratios))] %>% min %>% round(2)
following_ratio_max <- following_ratios[which(!is.na(following_ratios))] %>% max %>% round(2)

paste("Following ratio statistics:")
paste(length(which(following_ratios==0)), "are not following anyone.")
paste(length(which(is.na(following_ratios))), "have zero followers.")
paste("Mean =", following_ratio_mean)
paste("Standard Deviation =", following_ratio_sd)
paste("Median =", following_ratio_median)
paste("Range =", following_ratio_min, "to", following_ratio_max)
```   

Second, reconstruct threads of replies extending beyond #Edchat, starting by looking up tweets that have been replied to but are not in #Edchat.

```{r, include=FALSE}
## Just run once: very time consuming process. Takes 117 iterations, left with 291 unknown tweets.
#edchat_expanded <- edchat_no_rt
#new_tweets = unknown_replies
#i = 0; paste(i)
#
#while(nrow(new_tweets) > 0) {
#    i = i +1; print(i)
#    new_tweets = unknown_replies$reply_to_status_id %>% 
#        lookup_tweets() %>%
#        flatten()
#    edchat_expanded <- rbind(edchat_expanded, new_tweets) 
#    
#    all_replies <- edchat_expanded %>% filter(!is.na(reply_to_status_id))
#    unknown_replies <- all_replies %>% filter(!(reply_to_status_id %in% edchat_expanded$status_id))
#    print("New tweets:"); print(nrow(new_tweets))
#    print("Unknown replies:"); print(nrow(unknown_replies))
#}

#edchat_expanded_tmp <- edchat_expanded %>% filter(!is.na(created_at))
#edchat_expanded_new <- edchat_expanded %>% 
#    filter(is.na(created_at)) %>%
#    pull(status_id) %>% 
#    lookup_tweets() %>%
#    flatten() %>%
#    mutate(created_at = created_at %>% ymd_hms() %>% with_tz(tzone="US/Eastern"),
#           day = weekdays(created_at),
#           is_sync = ifelse(day == "Tuesday" &
#                                hour(created_at) >= 18 & 
#                                hour(created_at) < 21,
#                            TRUE,
#                            FALSE
#                             )
#           )
#edchat_expanded_tmp <- edchat_expanded_tmp %>% 
#    mutate_if(is.factor, as.character)
#edchat_expanded_new <- edchat_expanded_new %>% 
#    mutate_if(is.factor, as.character)

#edchat_expanded_to_save <- rbind(edchat_expanded_tmp, edchat_expanded_new)
#dim(edchat_expanded_to_save)

#write.csv(edchat_expanded_to_save, "edchat_expanded.csv", row.names=FALSE)
```

```{r, include=TRUE, echo=FALSE}
edchat_expanded_og <- read.csv("edchat_expanded.csv", 
                            header=TRUE, 
                            colClasses= c(status_id='character',
                                          reply_to_status_id='character',
                                          user_id='character',
                                          reply_to_user_id='character',
                                          text='character'
                                          )
                            ) %>%
    mutate(created_at = created_at %>% ymd_hms() %>% force_tz(tzone="US/Eastern"),
           day = weekdays(created_at),
           is_sync = ifelse(day == "Tuesday" &
                                hour(created_at) >= 18 & 
                                hour(created_at) < 21,
                            TRUE,
                            FALSE
                             )
           )
```


```{r, include=FALSE}
freq_replies_expanded <- table(edchat_expanded_og$reply_to_status_id) %>% 
    as.data.frame() %>% 
    rename(status_id = Var1,
           reply_count = Freq) %>%
    mutate(status_id = as.character(status_id)) %>% 
    arrange(desc(reply_count))
```

```{r, include=FALSE}
hashtag_regex <- "#([0-9]|[a-zA-Z])+"
url_regex <- "http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+"

edchat_expanded <- edchat_expanded_og %>% 
    left_join(freq_replies_expanded, by='status_id') %>% 
    mutate(reply_count = ifelse(is.na(reply_count), 0, reply_count),
           has_edchat = ifelse(str_detect(text, "#[Ee][Dd][Cc][Hh][Aa][Tt]"), TRUE, FALSE),
           in_thread = ifelse(!is.na(reply_to_status_id) | reply_count != 0, TRUE, FALSE),
           org = ifelse(has_edchat, 
                        ifelse(in_thread, "both", "hashtag"),
                        "thread"),
           is_head = ifelse(is.na(reply_to_status_id) & 
                             (status_id %in% edchat_expanded_og$reply_to_status_id),
                         TRUE, 
                         FALSE),
           is_tail = ifelse(!is.na(reply_to_status_id) & 
                             !(status_id %in% edchat_expanded_og$reply_to_status_id),
                         TRUE, 
                         FALSE),
           word_count = str_count(text, "\\s+") + 1,
           character_count = str_length(text),
           hashtag_count = str_count(text, hashtag_regex),
           url_count = str_count(text, url_regex)
           )
```

**Overall volume of tweets**

```{r, include=TRUE, echo=FALSE}
n_tweets_expanded <- edchat_expanded$status_id %>% unique() %>% length()
n_tweeters_expanded <- edchat_expanded$user_id %>% unique() %>% length()

paste("Number of unique tweets (no retweets):", n_tweets_expanded); paste("Number of unique tweeters (no retweets):", n_tweeters_expanded)

```

**Tweet volume by organizing feature**

```{r, include=TRUE, echo=FALSE}
volume_df <- table(edchat_expanded$org) %>% 
    as.data.frame() %>%
    rename(organized_by = Var1,
           count = Freq) %>%
    mutate(proportion = round(100*count/n_tweets_expanded, 2))
volume_df
```

**Time Frame**

```{r, include=TRUE, echo=FALSE}
time_start_expanded <- min(edchat_expanded$created_at)
time_end_expanded <- max(edchat_expanded$created_at)
n_months_expanded <- time_length(time_end_expanded - time_start_expanded, unit="months")
```

**Synchronous vs. Asynchronous Tweets per Hour**

```{r, include=TRUE, echo=FALSE}
n_weeks <- (time_end_expanded - time_start_expanded) %>% time_length(unit="weeks")
n_hours <- (time_end_expanded - time_start_expanded) %>% time_length(unit="hours")
sync_hours <- n_weeks * 3
async_hours <- n_hours - sync_hours

n_tues_tweets <- edchat_expanded %>% filter(day == "Tuesday") %>% nrow()
weekday_tweets <-  table(edchat_expanded$day) %>% as.data.frame()
tues_hours <- edchat_expanded %>% filter(day=="Tuesday") %>% 
    pull(created_at) %>% hour() %>% table() %>% as.data.frame()

n_sync_tweets <- edchat_expanded %>% filter(is_sync) %>% nrow()
n_async_tweets <- n_tweets_expanded - n_sync_tweets

hourly_all <- n_tweets_expanded / n_hours
hourly_sync <- n_sync_tweets / sync_hours
hourly_async <- n_async_tweets / async_hours
paste("Overall tweets per hour:", round(hourly_all, 2)); paste("Asynchronous tweets per hour:", round(hourly_async,2)); paste("Synchronous tweets per hour:", round(hourly_sync,2))
```

1. Compare the proportion of tweets during weekly *synchronous* chats across organizing feature.

```{r, include=TRUE, echo=FALSE}
sync_proportions <- edchat_expanded %>% 
    group_by(org) %>%
    summarize(sync_proportion = (100 * length(which(is_sync)) / 
                                     length(is_sync)) %>% round(2)
              ) %>%
    pull(sync_proportion) 
sync_chisq <- chisq.test(sync_proportions)

sync_row <- sync_proportions %>%
    t() %>%
    as.data.frame() %>%
    rename(both = V1, hashtag = V2, threads = V3) %>%
    mutate(stat = round(sync_chisq$statistic, 2),
           p = round(sync_chisq$p.value, 4)
           ) %>%
    select(hashtag, threads, both, stat, p)
rownames(sync_row) <- "during sync"
sync_row
```

```{r, include=TRUE, echo=TRUE}
## H = statistic obtained in the Kruskal-Wallis test
## k = number of groups
## n = total number of observations

eta.squared.es <- function(H, k, n, decimals=3) {
    round((H-k+1)/(n-k), decimals)
}
## eta-squared estimate assumes values from 0 to 1 and multiplied by 100% indicates the percentage of variance in the dependent variable explained by the independent variable


epsilon.squared.es <- function(H, n, decimals=3) {
    round(H / (((n^2) - 1) / (n + 1)), decimals)
}
## epsilon-squared coefficient assumes the value from 0 (indicating no relationship) to 1 (indicating a perfect relationship)

n_groups <- edchat_expanded$org %>% as.factor() %>% levels() %>% length()
n_obs <- edchat_expanded %>% nrow()
```

2. Compare the average number of *words* per tweet across organizing feature.

```{r, include=TRUE, echo=FALSE}
wordcount_means <- edchat_expanded %>% 
    group_by(org) %>%
    summarize(wordcount_means = mean(word_count) %>% round(2)
              ) %>%
    pull(wordcount_means) 
#boxplot(word_count ~ as.factor(org), data = edchat_expanded)

## Kruskal-Wallis non-parametric test to compare means
wordcount_kw <- kruskal.test(word_count ~ as.factor(org), 
                             data = edchat_expanded)

wordcount_row <- wordcount_means %>%
    t() %>%
    as.data.frame() %>%
    rename(both = V1, hashtag = V2, threads = V3) %>%
    mutate(stat = round(wordcount_kw$statistic, 2),
           p = round(wordcount_kw$p.value, 4), 
           eta_sq = eta.squared.es(H=as.numeric(wordcount_kw$statistic), 
                                   k=n_groups, 
                                   n=n_obs),
           eps_sq = epsilon.squared.es(H=as.numeric(wordcount_kw$statistic),
                                       n=n_obs)
           ) %>%
    select(hashtag, threads, both, stat, p,  eta_sq, eps_sq)
rownames(wordcount_row) <- "words"
wordcount_row
```

3. Look at the difference in average number of *characters* per tweet.

```{r, include=TRUE, echo=FALSE}
charcount_means <- edchat_expanded %>% 
    group_by(org) %>%
    summarize(charcount_means = mean(str_length(text)) %>% round(2)
              ) %>%
    pull(charcount_means) 
#boxplot(str_length(text) ~ as.factor(org), data = edchat_expanded)

## Kruskal-Wallis non-parametric test to compare means
charcount_kw <- kruskal.test(str_length(text) ~ as.factor(org), 
                             data = edchat_expanded)

char_row <- charcount_means %>%
    t() %>%
    as.data.frame() %>%
    rename(both = V1, hashtag = V2, threads = V3) %>%
    mutate(stat = round(charcount_kw$statistic, 2),
           p = round(charcount_kw$p.value, 4),
           eta_sq = eta.squared.es(H=as.numeric(charcount_kw$statistic), 
                                   k=n_groups, 
                                   n=n_obs),
           eps_sq = epsilon.squared.es(H=as.numeric(charcount_kw$statistic),
                                       n=n_obs)
           ) %>%
    select(hashtag, threads, both, stat, p,  eta_sq, eps_sq)
rownames(char_row) <- "characters"
char_row
```

4. Look at the difference in text-polarity *sentiment score* per tweet.

```{r, include=TRUE, echo=FALSE}
in_sentiment <- sentiment(replies_with_edchat$text) %>% 
    group_by(element_id) %>% 
    summarize(word_count = sum(word_count), 
              sentiment = sum(sentiment))
out_sentiment <- sentiment(replies_without_edchat$text) %>%
    group_by(element_id) %>% 
    summarize(word_count = sum(word_count), 
              sentiment = sum(sentiment))

m_in_sentiment <- mean(in_sentiment$sentiment)
m_out_sentiment <- mean(out_sentiment$sentiment)
sig_sentiment <- t.test(x=in_sentiment$sentiment,
                        y=out_sentiment$sentiment)
t_sentiment <- sig_sentiment$statistic %>% as.vector()
p_sentiment <- sig_sentiment$p.value %>% as.vector()
d_sentiment <- compute.es::tes(t_sentiment, 
                               n.1 = n_replies_with_edchat, 
                               n.2 = n_replies_without_edchat, verbose=FALSE)$d %>%
    abs()
row_sentiment <- c(m_in_sentiment, m_out_sentiment, t_sentiment, p_sentiment, d_sentiment)
```

5. Look at the difference in average number of *hashtags* per tweet.

```{r, include=TRUE, echo=FALSE}
m_in_hashtags <- mean(replies_with_edchat$hashtag_count)
m_out_hashtags <- mean(replies_without_edchat$hashtag_count)
sig_hashtags <- t.test(x=replies_with_edchat$hashtag_count,
                       y=replies_without_edchat$hashtag_count)
t_hashtags <- sig_hashtags$statistic %>% as.vector()
p_hashtags <- sig_hashtags$p.value %>% as.vector()
d_hashtags <- compute.es::tes(t_hashtags, 
                               n.1 = n_replies_with_edchat, 
                               n.2 = n_replies_without_edchat, verbose=FALSE)$d %>%
    abs()
row_hashtag <- c(m_in_hashtags, m_out_hashtags, t_hashtags, p_hashtags, d_hashtags)

hashtag_means <- edchat_expanded %>% 
    group_by(org) %>%
    summarize(means = mean(hashtag_count) %>% round(2),
              sds = sd(hashtag_count) %>% round(2),
              vars = var(hashtag_count)
              )
#boxplot(hashtag_count ~ as.factor(org), data = edchat_expanded)

library(car)
leveneTest(hashtag_count ~ as.factor(org), 
                             data = edchat_expanded)

## Kruskal-Wallis non-parametric test to compare means
hashtag_kw <- kruskal.test(hashtag_count ~ as.factor(org), 
                             data = edchat_expanded)
aov(hashtag_count ~ as.factor(org), 
                             data = edchat_expanded)
## I recommend that if you have non-normal data that can't be fixed by transformation, you go ahead and use one-way anova, but be cautious about rejecting the null hypothesis if the P value is not very far below 0.05 and your data are extremely non-normal.

hashtag_row <- hashtag_means %>%
    t() %>%
    as.data.frame() %>%
    rename(both = V1, hashtag = V2, threads = V3) %>%
    mutate(stat = round(hashtag_kw$statistic, 2),
           p = round(hashtag_kw$p.value, 4),
           eta_sq = eta.squared.es(H=as.numeric(hashtag_kw$statistic), 
                                   k=n_groups, 
                                   n=n_obs),
           eps_sq = epsilon.squared.es(H=as.numeric(hashtag_kw$statistic),
                                       n=n_obs)
           ) %>%
    select(hashtag, threads, both, stat, p,  eta_sq, eps_sq)
rownames(hashtag_row) <- "hashtags"
hashtag_row

```

5. Look at the difference in average number of *hyperlinks* per tweet.

```{r, include=TRUE, echo=FALSE}
m_in_hyperlinks <- mean(replies_with_edchat$url_count)
m_out_hyperlinks <- mean(replies_without_edchat$url_count)
sig_hyperlinks <- t.test(x=replies_with_edchat$url_count,
                       y=replies_without_edchat$url_count)
t_hyperlinks <- sig_hyperlinks$statistic %>% as.vector()
p_hyperlinks <- sig_hyperlinks$p.value %>% as.vector()
d_hyperlinks <- compute.es::tes(t_hyperlinks, 
                               n.1 = n_replies_with_edchat, 
                               n.2 = n_replies_without_edchat, verbose=FALSE)$d %>%
    abs()
row_hyperlinks <- c(m_in_hyperlinks, m_out_hyperlinks, t_hyperlinks, p_hyperlinks, d_hyperlinks)
```

6. Look at the difference in average number of *likes* per tweet.

```{r, include=TRUE, echo=FALSE}
m_in_likes <- mean(replies_with_edchat$favorite_count)
m_out_likes <- mean(replies_without_edchat$favorite_count)
sig_likes <- t.test(x=replies_with_edchat$favorite_count,
                    y=replies_without_edchat$favorite_count)
t_likes <- sig_likes$statistic %>% as.vector()
p_likes <- sig_likes$p.value %>% as.vector()
d_likes <- compute.es::tes(t_likes, 
                               n.1 = n_replies_with_edchat, 
                               n.2 = n_replies_without_edchat, verbose=FALSE)$d %>%
    abs()
row_likes <- c(m_in_likes, m_out_likes, t_likes, p_likes, d_likes)
```

7. Look at the difference in average number of *retweets* per tweet.

```{r, include=TRUE, echo=FALSE}
m_in_retweets <- mean(replies_with_edchat$retweet_count)
m_out_retweets <- mean(replies_without_edchat$retweet_count)
sig_retweets <- t.test(x=replies_with_edchat$retweet_count,
                    y=replies_without_edchat$retweet_count)
t_retweets <- sig_retweets$statistic %>% as.vector()
p_retweets <- sig_retweets$p.value %>% as.vector()
d_retweets <- compute.es::tes(t_retweets, 
                               n.1 = n_replies_with_edchat, 
                               n.2 = n_replies_without_edchat, verbose=FALSE)$d %>%
    abs()
row_retweets <- c(m_in_retweets, m_out_retweets, t_retweets, p_retweets, d_retweets)
```

8. Look at the difference in average number of *replies* per tweet.

```{r, include=TRUE, echo=FALSE}
m_in_replies <- mean(replies_with_edchat$reply_count)
m_out_replies <- mean(replies_without_edchat$reply_count)
sig_replies <- t.test(x=replies_with_edchat$reply_count,
                    y=replies_without_edchat$reply_count)
t_replies <- sig_replies$statistic %>% as.vector()
p_replies <- sig_replies$p.value %>% as.vector()
d_replies <- compute.es::tes(t_replies, 
                               n.1 = n_replies_with_edchat, 
                               n.2 = n_replies_without_edchat, verbose=FALSE)$d %>%
    abs()
row_replies <- c(m_in_replies, m_out_replies, t_replies, p_replies, d_replies)
```

```{r, include=TRUE, echo=FALSE}
results_table <- row_word %>%
    rbind(row_character) %>% 
    rbind(row_sentiment) %>%
    rbind(row_hashtag) %>% 
    rbind(row_hyperlinks) %>% 
    rbind(row_likes) %>% 
    rbind(row_retweets) %>% 
    rbind(row_replies) %>%
    as.data.frame() %>%
    round(4)
rownames(results_table) <- c("Words", "Characters", "Sentiment",
                             "Hashtags", "Hyperlinks",
                             "Likes", "Retweets", "Replies")
colnames(results_table) <- c("Mean (in #Edchat)", "Mean (outside #Edchat)",
                             "t", "p", "d")
results_table
#write.csv(results_table, "results_table.csv", row.names=TRUE)
```











**What proportion of replies contain the #Edchat hashtag?**

```{r, include=TRUE, echo=FALSE}
all_replies <- edchat_expanded %>% filter(!is.na(reply_to_status_id))
known_replies <- all_replies %>% 
    filter(reply_to_status_id %in% edchat_expanded$status_id)
unknown_replies <- all_replies %>% 
    filter(!(reply_to_status_id %in% edchat_expanded$status_id))

paste("Total tweets:", nrow(edchat_expanded)); paste("All replies:", nrow(all_replies)); paste("Known replies:", nrow(known_replies)); paste("Unavailable replies:", nrow(unknown_replies))
```

**Frequency of replies with keyword #edchat vs. without**

```{r, include=TRUE, echo=FALSE}
n_all_replies <- nrow(all_replies)
n_not_replies <- nrow(edchat_expanded) - n_all_replies
n_replies_with_edchat <- nrow(filter(all_replies, in_edchat==TRUE))
n_replies_without_edchat <- nrow(filter(all_replies, in_edchat==FALSE))
p_replies_with_edchat <- round(100 * n_replies_with_edchat / nrow(all_replies), 2)

n_all_with_edchat <- nrow(filter(edchat_expanded, in_edchat==TRUE))
n_all_without_edchat <- nrow(filter(edchat_expanded, in_edchat==FALSE))

p_in_replies <- 100 * n_replies_with_edchat / n_all_with_edchat
p_out_replies <- 100 * n_replies_without_edchat / n_all_without_edchat

paste("There are", n_all_replies, "tweets in the #Edchat conversation.", n_replies_with_edchat, "of these (", p_replies_with_edchat, "% ) contain the keyword text #edchat.")
```

**Create a contingency table and test significance of differences.**

```{r, include=TRUE, echo=FALSE}
matrix_replies <- matrix(c(n_replies_with_edchat,
                           n_replies_without_edchat,
                           n_all_with_edchat - n_replies_with_edchat,
                           n_all_without_edchat - n_replies_without_edchat
                           ), 
                         ncol=2,
                         dimnames = list(c("with #edchat", "without #edchat"),
                                         c("reply", "original tweet"))
                         )
sig_replies <- chisq.test(matrix_replies)
x2_replies  <- sig_replies$statistic %>% as.vector()
df_replies <- sig_replies$parameter %>% as.vector()
p_replies  <- sig_replies$p.value %>% as.vector()
d_replies  <- compute.es::chies(x2_replies, n = nrow(edchat_expanded))$d %>% abs()

paste("Tweets containing the keyword #edchat were replies", round(p_in_replies,2), "% of the time. Tweets missing the hashtag were replies ", round(p_out_replies,2), "% of the time. The two-way table test of association between the presence of the keyword #edchat and tweet type had a chi-square value of", round(x2_replies,2), ", (df =", df_replies, " ), p = ", round(p_replies,4), "and an effect size d =", round(d_replies,2), ".")
```

Print the contingency table and the results of the chi-square test. 

```{r, include=TRUE, echo=FALSE}
matrix_replies; sig_replies
```

**In the #Edchat reply network, how do tweets with the keyword #edchat compare to those missing the hashtag?**

```{r, include=FALSE}
replies_with_edchat <- edchat_expanded %>% 
    filter(!is.na(reply_to_status_id), in_edchat==TRUE)
replies_without_edchat <- edchat_expanded %>% 
    filter(!is.na(reply_to_status_id), in_edchat==FALSE)
```






**Frequency of self-replies with keyword #edchat vs. without**

```{r, include=TRUE, echo=FALSE}
n_in_self_replies <- replies_with_edchat %>%
    filter(user_id == reply_to_user_id) %>%
    nrow()
p_in_self_replies <- 100 * n_in_self_replies / n_replies_with_edchat

n_out_self_replies <- replies_without_edchat %>%
    filter(user_id == reply_to_user_id) %>%
    nrow()
p_out_self_replies <- 100 * n_out_self_replies / n_replies_without_edchat
```

Create a contingency table and test significance of differences.

```{r, include=TRUE, echo=FALSE}
matrix_self_replies <- matrix(c(n_in_self_replies,
                                n_out_self_replies,
                                n_replies_with_edchat - n_in_self_replies,
                                n_replies_without_edchat - n_out_self_replies
                                ), 
                              ncol=2,
                              dimnames = list(c("with #edchat", "without #edchat"),
                                              c("self-reply", "reply to others"))
                              )

sig_self_replies <- chisq.test(matrix_self_replies)
x2_self_replies  <- sig_self_replies$statistic %>% as.vector()
df_self_replies <- sig_self_replies$parameter %>% as.vector()
p_self_replies  <- sig_self_replies$p.value %>% as.vector()
d_self_replies  <- compute.es::chies(x2_self_replies, n = n_all_replies)$d %>% abs()

paste("Replies containing the keyword #edchat were self-replies", round(p_in_self_replies,2), "% of the time. Replies missing the hashtag were self-replies ", round(p_out_self_replies,2), "% of the time. The two-way table test of association between the presence of the keyword #edchat and tweet type had a chi-square value of", round(x2_self_replies,2), ", (df =", df_self_replies, " ), p = ", round(p_self_replies,4), "and an effect size d =", round(d_self_replies,2), ".")
```

Print the contingency table and the results of the chi-square test. 

```{r, include=TRUE, echo=FALSE}
matrix_self_replies; sig_self_replies
```

The p-value is significant for the two-way contingency table chi-square test, indicating that some association between the variables is present. I conclude that the increased likelihood of self-replies in tweets not containing the keyword "#edchat" emphasis is not due to random variation. 

```{r, include=TRUE, echo=FALSE}
repliers_with_edchat <- replies_with_edchat %>% 
    distinct(user_id, .keep_all = TRUE) %>%
    select(user_id, screen_name, 
           place_url, place_name, place_full_name, place_type, 
           country, country_code, geo_coords, coords_coords, bbox_coords,
           name, location, description, url, 
           followers_count, friends_count, listed_count, statuses_count, favourites_count,
           account_created_at, verified, profile_url, profile_expanded_url, 
           account_lang
           )
replies_with_edchat %>% nrow(); repliers_with_edchat %>% nrow()
```

```{r, include=TRUE, echo=FALSE}
repliers_without_edchat <- replies_without_edchat %>% 
    distinct(user_id, .keep_all = TRUE) %>%
    select(user_id, screen_name, 
           place_url, place_name, place_full_name, place_type, 
           country, country_code, geo_coords, coords_coords, bbox_coords,
           name, location, description, url, 
           followers_count, friends_count, listed_count, statuses_count, favourites_count,
           account_created_at, verified, profile_url, profile_expanded_url, 
           account_lang
           )
replies_without_edchat %>% nrow(); repliers_without_edchat %>% nrow()
```

```{r, include=TRUE, echo=FALSE}
n_all_repliers <- length(unique(all_replies$user_id))
paste("The #Edchat reply network contained", n_all_replies, "replies from", n_all_repliers, "repliers.")
```

```{r, include=TRUE, echo=FALSE}
`%notin%` <- Negate(`%in%`)

repliers_both <- repliers_with_edchat %>% 
    filter(user_id %in% repliers_without_edchat$user_id)
repliers_with_only <- repliers_with_edchat %>%
    filter(user_id %notin% repliers_without_edchat$user_id)
repliers_without_only <- repliers_without_edchat %>% 
    filter(user_id %notin% repliers_with_edchat$user_id)
nrow(repliers_both); nrow(repliers_with_only); nrow(repliers_without_only)

paste(nrow(repliers_with_only), "contributors to the #Edchat reply network always included the #Edchat hashtag in their replies;", nrow(repliers_both), "contributors sometimes included the hashtag, sometimes not;", "and", nrow(repliers_without_only), "contributors never included the #Edchat hashtag in their replies.")
```




Next, create the network graph of #Edchat replies using the `igraph` package.

```{r, include=TRUE, echo=FALSE}
length(unique(c(all_replies$user_id, all_replies$reply_to_user_id)))
length(c(all_replies$user_id, all_replies$reply_to_user_id))


freq_all_reply_nodes <- all_replies$user_id %>% 
    c(all_replies$reply_to_user_id) %>% 
    table() %>% 
    as.data.frame() %>% 
    rename(user_id = ".",
           count = Freq) %>%
    mutate(user_id = as.character(user_id)) %>% 
    arrange(desc(count))

edge_in_edchat <- all_replies %>%
    mutate(in_edchat = ifelse(in_edchat, 1, 0)) %>%
    pull(in_edchat)

reply_graph <- all_replies %>% 
    select(user_id, reply_to_user_id) %>%
    as.matrix() %>%
    graph_from_edgelist(directed=TRUE) %>% 
    set_vertex_attr(name='degree', value=degree(., mode='total', loops=FALSE)) %>%
    set_edge_attr(name='in_out', value=edge_in_edchat)  #edge.betweenness()
reply_graph %>% summary()
```

Then calculate network statistics.

```{r, include=TRUE}
## size
reply_graph %>% V() %>% length()  # number of vertices/nodes
reply_graph %>% gsize()  # number of edges
#reply_graph %>% diameter(directed=TRUE, unconnected=TRUE)  # the length of the longest geodesic (max distance between two vertices)

round(reply_graph %>% transitivity("global") * 100, 2)  
## The balance of connections. Also called the clustering coefficient.
## The probability that the adjacent vertices of a vertex are connected. 
## When the clustering coefficient is large it implies that a graph is highly clustered around a few nodes, 
## When it is low it implies that the links in the graph are relatively evenly spread among all the nodes. (Hogan, 2017)


round(reply_graph %>% reciprocity * 100, 2)  
## The proportion of mutual connections (in a directed network).
## The probability that the opposite counterpart of a directed edge is also included in the graph.


## average connectedness
reply_graph %>% vertex_attr('degree') %>% mean() %>% round(2)
reply_graph %>% vertex_attr('degree') %>% sd() %>% round(2)
reply_graph %>% vertex_attr('degree') %>% median()
reply_graph %>% vertex_attr('degree') %>% min()
reply_graph %>% vertex_attr('degree') %>% max()

reply_graph %>% edge_attr('in_out') %>% mean() %>% round(2)
```

Visualization with Gephi

```{r, include=TRUE}
replies_edgelist3 <- all_replies %>% 
    select(user_id, reply_to_user_id, in_edchat) %>%
    filter(user_id != reply_to_user_id) %>% 
    as.data.frame() %>%
    rename(Source = user_id,
           Target = reply_to_user_id
           ) %>%
    mutate(Source = pmin(Source, Target),
           Target = pmax(Source, Target)
           ) %>% 
    group_by(Source) %>% mutate(Source_n = n()) %>% ungroup() %>%
    group_by(Target) %>% mutate(Target_n = n()) %>% ungroup() %>%
    group_by(Source, Target) %>% mutate(edge_n = n())
nrow(replies_edgelist3)
#write.csv(replies_edgelist3, "replies_edgelist3.csv", row.names=FALSE)
```

Visualization with igraph in R

```{r, include=TRUE}
reply_graph_viz <- known_replies %>% 
    select(user_id, reply_to_user_id) %>%
    filter(user_id != reply_to_user_id) %>%
    as.matrix() %>% 
    graph_from_edgelist(directed=TRUE) %>% 
    set_vertex_attr(name='degree', value=degree(., mode='total', loops=FALSE))
reply_graph_viz %>% summary()
reply_graph %>% summary()

layout <- reply_graph_viz %>% create_layout(layout = 'drl') 

ggraph(layout) +
        geom_edge_link(alpha = .2, width=.3) +
        geom_node_point(alpha=.2, size=1) +
        theme_bw() +  # makes background white (not gray)
        theme(plot.background = element_blank(),
              panel.grid.major = element_blank(),
              panel.grid.minor = element_blank(),
              panel.border = element_blank(),
              axis.title = element_blank(),
              axis.text = element_blank(),
              axis.ticks = element_blank(),
              legend.position="none"
        )
#ggsave("r-output/edchat_network_visualization_in_R.png", width = 1 * 10, height = 1 * 10)
```


## RQ3: What are participants talking about with each other in #Edchat?

Take samples (n = 100) of replies and repliers with keyword #edchat and without.

```{r, include=TRUE, echo=FALSE}
set.seed(6292019)
sample_replies_with_edchat <- sample_n(replies_with_edchat, 100)
#write.csv(sample_replies_with_edchat, "sample_replies_with_edchat.csv", row.names=FALSE)
sample_replies_without_edchat <- sample_n(replies_without_edchat, 100)
#write.csv(sample_replies_without_edchat, "sample_replies_without_edchat.csv", row.names=FALSE)
sample_repliers_with_edchat <- sample_n(repliers_with_edchat, 100)
#write.csv(sample_repliers_with_edchat, "sample_repliers_with_edchat.csv", row.names=FALSE)
sample_repliers_without_edchat <- sample_n(repliers_without_edchat, 100)
#write.csv(sample_repliers_without_edchat, "sample_repliers_without_edchat", row.names=FALSE)
```



```{r, include=FALSE}
coded_sample_with_og <- read.csv("sample_replies_with_edchat_coded.csv", 
                              header=TRUE, 
                              colClasses= c(status_id='character',
                                            reply_to_status_id='character',
                                            user_id='character',
                                            reply_to_user_id='character',
                                            text='character'
                                            )
                              )
coded_sample_without_og <- read.csv("sample_replies_without_edchat_coded.csv", 
                                 header=TRUE, 
                                 colClasses= c(status_id='character',
                                               reply_to_status_id='character',
                                               user_id='character',
                                               reply_to_user_id='character',
                                               text='character'
                                               )
                                 )
```

```{r, include=FALSE}
coded_sample_with <- coded_sample_with_og %>%
    mutate(self = ifelse(is.na(self), 0, 1),
           others = ifelse(is.na(others), 0, 1),
           mutual = ifelse(is.na(mutual), 0, 1),
           misc = ifelse(is.na(misc), 0, 1),
           cognitive = ifelse(is.na(cognitive), 0, 1),
           interactive = ifelse(is.na(interactive), 0, 1),
           social = ifelse(is.na(social), 0, 1)
           )
coded_sample_without <- coded_sample_without_og %>%
    mutate(self = ifelse(is.na(self), 0, 1),
           others = ifelse(is.na(others), 0, 1),
           mutual = ifelse(is.na(mutual), 0, 1),
           misc = ifelse(is.na(misc), 0, 1),
           cognitive = ifelse(is.na(cognitive), 0, 1),
           interactive = ifelse(is.na(interactive), 0, 1),
           social = ifelse(is.na(social), 0, 1)
           )
```

```{r, include=TRUE, echo=FALSE}
purpose_sums_with <- coded_sample_with %>%
    summarize(self = sum(self),
              others = sum(others),
              mutual = sum(mutual),
              miscellaneous = sum(misc)
              )
purpose_sums_without <- coded_sample_without %>%
    summarize(self = sum(self),
              others = sum(others),
              mutual = sum(mutual),
              miscellaneous = sum(misc)
              )
matrix_purpose_sums <- purpose_sums_with %>%
    rbind(purpose_sums_without) %>%
    t() %>%
    as.matrix()
colnames(matrix_purpose_sums) <- c("with", "without")

sig_purpose <- chisq.test(matrix_purpose_sums)
x2_purpose  <- sig_purpose$statistic %>% as.vector()
df_purpose <- sig_purpose$parameter %>% as.vector()
p_purpose  <- sig_purpose$p.value %>% as.vector()
d_purpose  <- compute.es::chies(x2_purpose, n = 200)$d %>% abs()

paste("The two-way table test of association between the presence of the keyword #edchat and tweet purpose had a chi-square value of", round(x2_purpose,2), ", (df =", df_purpose, "), p = ", round(p_purpose,4), "and an effect size d =", round(d_purpose,2), ".")
```

Print the contingency table and the results of the chi-square test. 

```{r, include=TRUE, echo=FALSE}
matrix_purpose_sums; sig_purpose
```

```{r, include=TRUE, echo=FALSE}
discourse_sums_with <- coded_sample_with %>%
    summarize(cognitive = sum(cognitive),
              interactive = sum(interactive),
              social = sum(social)
              )
discourse_sums_without <- coded_sample_without %>%
    summarize(cognitive = sum(cognitive),
              interactive = sum(interactive),
              social = sum(social)
              )
matrix_discourse_sums <- discourse_sums_with %>%
    rbind(discourse_sums_without) %>%
    t() %>%
    as.matrix()
colnames(matrix_discourse_sums) <- c("with", "without")

sig_discourse <- chisq.test(matrix_discourse_sums)
x2_discourse  <- sig_discourse$statistic %>% as.vector()
df_discourse <- sig_discourse$parameter %>% as.vector()
p_discourse  <- sig_discourse$p.value %>% as.vector()
d_discourse  <- compute.es::chies(x2_discourse, n = 200)$d %>% abs()

paste("The two-way table test of association between the presence of the keyword #edchat and tweet discourse had a chi-square value of", round(x2_discourse,2), ", (df =", df_discourse, "), p = ", round(p_discourse,4), "and an effect size d =", round(d_discourse,2), ".")
```

Print the contingency table and the results of the chi-square test. 

```{r, include=TRUE, echo=FALSE}
matrix_discourse_sums; sig_discourse
```



**Visualize these differences in tweet purpose and tweet discourse in replies with and without #Edchat**

First, calculate margins of error (moe)


```{r, include=FALSE}
# for 95% confidence level, sample size = 100, rounded to 4 digits
moe <- function(x, n=100, z=1.96, dig=4) {
   x = x / 100 
   y = round(z * sqrt(x * (1 - x) / n), digits=dig)
   return(100 * y)
}
```

**Purpose**

```{r, includeFALSE}
purposes <- rep(rownames(matrix_purpose_sums), 2)
values_purp <- c(matrix_purpose_sums[,1], matrix_purpose_sums[,2])
purpose_moe <- moe(matrix_purpose_sums)
moe_purp <-c(purpose_moe[,1], purpose_moe[,2]) 
in_or_out_purp <- factor(c(rep(colnames(matrix_purpose_sums)[1], 4),
                           rep(colnames(matrix_purpose_sums)[2], 4)),
                         levels=colnames(matrix_purpose_sums), ordered=TRUE)
mydata_purp <- data.frame(purposes, values_purp, moe_purp, in_or_out_purp) %>%
    mutate(purposes = factor(purposes, levels=c(rownames(matrix_purpose_sums))))
mydata_purp
```

Now, display the plot.

```{r, include=TRUE, echo=FALSE}
ggplot(data=mydata_purp, 
       aes(x=purposes, y=values_purp, fill=in_or_out_purp)
       ) +
    geom_col(colour="grey20", 
             width = 0.5,
             position = position_dodge(width=0.5)
             ) +
    geom_errorbar(data=mydata_purp,
                  aes(ymin = values_purp - moe_purp, 
                      ymax = values_purp + moe_purp), 
                  width = 0.2, 
                  position = position_dodge(width=0.5)
                  ) +
    scale_fill_brewer(palette="Set1") +
    theme(panel.background = element_rect(fill = "white", colour = "white"),
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank(),
          panel.border = element_blank(),
          axis.line = element_line(size = .5, colour = "grey80"),
          axis.title=element_text(size=28, family="serif"),
          axis.text=element_text(size=22, family="serif"),
          legend.title=element_text(size=28, family="serif"), 
          legend.text=element_text(size=22, family="serif")
          ) +
    xlab("Tweet Purposes") + ylab("Proportion") + labs(fill="Keyword #Edchat") +
    geom_hline(yintercept=0, color="black", size = .75)
#ggsave("tweet_purpose_with_without.png", width = 1 * 16, height = 1 * 9)
```

**Discourse**

```{r, includeFALSE}
discourses <- rep(rownames(matrix_discourse_sums), 2)
values_disc <- c(matrix_discourse_sums[,1], matrix_discourse_sums[,2])
discourse_moe <- moe(matrix_discourse_sums)
moe_disc <-c(discourse_moe[,1], discourse_moe[,2]) 
in_or_out_disc <- factor(c(rep(colnames(matrix_discourse_sums)[1], 3),
                           rep(colnames(matrix_discourse_sums)[2], 3)),
                         levels=colnames(matrix_discourse_sums), ordered=TRUE)
mydata_disc <- data.frame(discourses, values_disc, moe_disc, in_or_out_disc) %>%
    mutate(discourses = factor(discourses, levels=c(rownames(matrix_discourse_sums))))
mydata_disc
```

Now, display the plot.

```{r, include=TRUE, echo=FALSE}
ggplot(data=mydata_disc, 
       aes(x=discourses, y=values_disc, fill=in_or_out_disc)
       ) +
    geom_col(colour="grey20", 
             width = 0.5,
             position = position_dodge(width=0.5)
             ) +
    geom_errorbar(data=mydata_disc,
                  aes(ymin = values_disc - moe_disc, 
                      ymax = values_disc + moe_disc), 
                  width = 0.2, 
                  position = position_dodge(width=0.5)
                  ) +
    scale_fill_brewer(palette="Set1") +
    theme(panel.background = element_rect(fill = "white", colour = "white"),
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank(),
          panel.border = element_blank(),
          axis.line = element_line(size = .5, colour = "grey80"),
          axis.title=element_text(size=28, family="serif"),
          axis.text=element_text(size=22, family="serif"),
          legend.title=element_text(size=28, family="serif"), 
          legend.text=element_text(size=22, family="serif")
          ) +
    xlab("Tweet Discourses") + ylab("Proportion") + labs(fill="Keyword #Edchat") +
    geom_hline(yintercept=0, color="black", size = .75)
#ggsave("tweet_discourse_with_without.png", width = 1 * 16, height = 1 * 9)
```



```{r, include=TRUE, echo=FALSE}
matrix_discourse_sums; moe(matrix_discourse_sums)
```



# Version/dependencies

```{r, session-info}
sessionInfo()
```

