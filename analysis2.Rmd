---
title: "Twitter Edchat Inbteractions Project Analysis"
author: "K. Bret Staudt Willet"
date: "8/01/2019"
output: 
    html_document:
        toc: true
        float_toc: true
---

# Get set up

This section loads the data and packages and starts to process the data, but doesn't calculate any statistics or create any results.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
usethis::use_git_ignore(c("*.csv", "*.rds"))
```

## Load packages

```{r, include=FALSE}
library(tidyverse)
library(lubridate)
library(rtweet)
library(compute.es)
library(igraph)
```

## Load the data

Having completed the steps in the setup.Rmd file, you now have the dataset stored in your local repository and can load it as usual. This project uses Twitter #Edchat data that have been run through the `rtweet` R package, which queries the Twitter API to return the most complete set of tweet metadata available, while also removing deleted and protected tweets. See https://rtweet.info/ for details on `rtweet`.

```{r, include=TRUE, echo=FALSE}
edchat_expanded <- read.csv("edchat_expanded2.csv", 
                            header=TRUE, 
                            colClasses= c(status_id='character',
                                          reply_to_status_id='character',
                                          user_id='character',
                                          reply_to_user_id='character',
                                          text='character',
                                          day='character',
                                          org='character'
                                          )
                            ) %>%
    mutate(created_at = created_at %>% ymd_hms() %>% force_tz(tzone="US/Eastern")
           )
```

```{r, include=TRUE, echo=FALSE}
edchat_expanded2 <- edchat_expanded %>% mutate(org = as.factor(org))

m1 <- glm(word_count ~ org,
          data = edchat_expanded2,
          family = 'poisson')
summary(m1)
## will give you p-values for the contrasts between hashtag, thread, both 

summary(aov(m1))
## will essentially do a likelihood ratio test to decide if the predictor `org` is supported at all (note that adding `org`, you are adding 2 df / parameters at once). 

car::Anova(m1)

word_poiss_aov <- poisson.anova(edchat_expanded2$word_count, edchat_expanded2$org) %>%
    t %>% as.data.frame()

reply_poiss_aov <- poisson.anova(y=edchat_expanded2$reply_count,
                                ina=edchat_expanded2$org)

hash_poiss_aov <- poisson.anova(y=edchat_expanded2$hashtag_count,
                                ina=edchat_expanded2$org)

like_poiss_aov <- poisson.anova(y=edchat_expanded2$favorite_count,
                                ina=edchat_expanded2$org)



retweet_poiss_aov <- poisson.anova(y=edchat_expanded2$retweet_count,
                                ina=edchat_expanded2$org)


m2 <- glm(reply_count ~ 1 + 
              #scale(hashtag_count) +
              #scale(favorite_count) +
              #scale(word_count) +
              org,
          data = edchat_expanded2,
          family = 'poisson')
summary(m2)

m3 <- glm(reply_count ~ 1 + 
              #scale(hashtag_count) +
              scale(favorite_count) +
              #scale(word_count) +
              org,
          data = edchat_expanded,
          family = 'poisson')
summary(m3)

m4 <- glm(reply_count ~ 1 + 
              scale(hashtag_count) +
              scale(favorite_count) +
              #scale(word_count) +
              org,
          data = edchat_expanded,
          family = 'poisson')
summary(m4)

m5 <- glm(reply_count ~ 1 + 
              scale(hashtag_count) +
              scale(favorite_count) +
              scale(word_count) +
              org,
          data = edchat_expanded,
          family = 'poisson')
summary(m5)

poisson.anova()
```







```{r, include=FALSE}
hashtag_regex <- "#([0-9]|[a-zA-Z])+"
url_regex <- "http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+"
```

**Overall volume of tweets**

```{r, include=TRUE, echo=FALSE}
n_tweets_expanded <- edchat_expanded$status_id %>% unique() %>% length()
n_tweeters_expanded <- edchat_expanded$user_id %>% unique() %>% length()

paste("Number of unique tweets (no retweets):", n_tweets_expanded); paste("Number of unique tweeters (no retweets):", n_tweeters_expanded)

```

**Tweet volume by organizing feature**

```{r, include=TRUE, echo=FALSE}
volume_df <- table(edchat_expanded$org) %>% 
    as.data.frame() %>%
    rename(organized_by = Var1,
           count = Freq) %>%
    mutate(proportion = round(100*count/n_tweets_expanded, 2))
volume_df
```

**Time Frame**

```{r, include=TRUE, echo=FALSE}
time_start_expanded <- min(edchat_expanded$created_at)
time_end_expanded <- max(edchat_expanded$created_at)
n_months_expanded <- time_length(time_end_expanded - time_start_expanded, unit="months")
```

**Synchronous vs. Asynchronous Tweets per Hour**

```{r, include=TRUE, echo=FALSE}
n_weeks <- (time_end_expanded - time_start_expanded) %>% time_length(unit="weeks")
n_hours <- (time_end_expanded - time_start_expanded) %>% time_length(unit="hours")
sync_hours <- n_weeks * 3
async_hours <- n_hours - sync_hours

n_tues_tweets <- edchat_expanded %>% filter(day == "Tuesday") %>% nrow()
weekday_tweets <-  table(edchat_expanded$day) %>% as.data.frame()
tues_hours <- edchat_expanded %>% filter(day=="Tuesday") %>% 
    pull(created_at) %>% hour() %>% table() %>% as.data.frame()

n_sync_tweets <- edchat_expanded %>% filter(is_sync) %>% nrow()
n_async_tweets <- n_tweets_expanded - n_sync_tweets

hourly_all <- n_tweets_expanded / n_hours
hourly_sync <- n_sync_tweets / sync_hours
hourly_async <- n_async_tweets / async_hours
paste("Overall tweets per hour:", round(hourly_all, 2)); paste("Asynchronous tweets per hour:", round(hourly_async,2)); paste("Synchronous tweets per hour:", round(hourly_sync,2))
```

1. Compare the proportion of tweets during weekly *synchronous* chats across organizing feature.

```{r, include=TRUE, echo=FALSE}
sync_proportions <- edchat_expanded %>% 
    group_by(org) %>%
    summarize(sync_proportion = (100 * length(which(is_sync)) / 
                                     length(is_sync)) %>% round(2)
              ) %>%
    pull(sync_proportion) 
sync_chisq <- chisq.test(sync_proportions)

sync_row <- sync_proportions %>%
    t() %>%
    as.data.frame() %>%
    rename(both = V1, hashtag = V2, threads = V3) %>%
    mutate(stat = round(sync_chisq$statistic, 2),
           p = round(sync_chisq$p.value, 4)
           ) %>%
    select(hashtag, threads, both, stat, p)
rownames(sync_row) <- "during sync"
sync_row
```

```{r, include=TRUE, echo=TRUE}
## H = statistic obtained in the Kruskal-Wallis test
## k = number of groups
## n = total number of observations

eta.squared.es <- function(H, k, n, decimals=3) {
    round((H-k+1)/(n-k), decimals)
}
## eta-squared estimate assumes values from 0 to 1 and multiplied by 100% indicates the percentage of variance in the dependent variable explained by the independent variable


epsilon.squared.es <- function(H, n, decimals=3) {
    round(H / (((n^2) - 1) / (n + 1)), decimals)
}
## epsilon-squared coefficient assumes the value from 0 (indicating no relationship) to 1 (indicating a perfect relationship)

n_groups <- edchat_expanded$org %>% as.factor() %>% levels() %>% length()
n_obs <- edchat_expanded %>% nrow()
```

2. Compare the average number of *words* per tweet across organizing feature.

```{r, include=TRUE, echo=FALSE}
wordcount_means <- edchat_expanded %>% 
    group_by(org) %>%
    summarize(wordcount_means = mean(word_count) %>% round(2)
              ) %>%
    pull(wordcount_means) 
#boxplot(word_count ~ as.factor(org), data = edchat_expanded)

## Kruskal-Wallis non-parametric test to compare means
wordcount_kw <- kruskal.test(word_count ~ as.factor(org), 
                             data = edchat_expanded)

wordcount_row <- wordcount_means %>%
    t() %>%
    as.data.frame() %>%
    rename(both = V1, hashtag = V2, threads = V3) %>%
    mutate(stat = round(wordcount_kw$statistic, 2),
           p = round(wordcount_kw$p.value, 4), 
           eta_sq = eta.squared.es(H=as.numeric(wordcount_kw$statistic), 
                                   k=n_groups, 
                                   n=n_obs),
           eps_sq = epsilon.squared.es(H=as.numeric(wordcount_kw$statistic),
                                       n=n_obs)
           ) %>%
    select(hashtag, threads, both, stat, p,  eta_sq, eps_sq)
rownames(wordcount_row) <- "words"
wordcount_row
```

3. Look at the difference in average number of *characters* per tweet.

```{r, include=TRUE, echo=FALSE}
charcount_means <- edchat_expanded %>% 
    group_by(org) %>%
    summarize(charcount_means = mean(str_length(text)) %>% round(2)
              ) %>%
    pull(charcount_means) 
#boxplot(str_length(text) ~ as.factor(org), data = edchat_expanded)

## Kruskal-Wallis non-parametric test to compare means
charcount_kw <- kruskal.test(str_length(text) ~ as.factor(org), 
                             data = edchat_expanded)

char_row <- charcount_means %>%
    t() %>%
    as.data.frame() %>%
    rename(both = V1, hashtag = V2, threads = V3) %>%
    mutate(stat = round(charcount_kw$statistic, 2),
           p = round(charcount_kw$p.value, 4),
           eta_sq = eta.squared.es(H=as.numeric(charcount_kw$statistic), 
                                   k=n_groups, 
                                   n=n_obs),
           eps_sq = epsilon.squared.es(H=as.numeric(charcount_kw$statistic),
                                       n=n_obs)
           ) %>%
    select(hashtag, threads, both, stat, p,  eta_sq, eps_sq)
rownames(char_row) <- "characters"
char_row
```

4. Look at the difference in text-polarity *sentiment score* per tweet.

```{r, include=TRUE, echo=FALSE}
in_sentiment <- sentiment(replies_with_edchat$text) %>% 
    group_by(element_id) %>% 
    summarize(word_count = sum(word_count), 
              sentiment = sum(sentiment))
out_sentiment <- sentiment(replies_without_edchat$text) %>%
    group_by(element_id) %>% 
    summarize(word_count = sum(word_count), 
              sentiment = sum(sentiment))

m_in_sentiment <- mean(in_sentiment$sentiment)
m_out_sentiment <- mean(out_sentiment$sentiment)
sig_sentiment <- t.test(x=in_sentiment$sentiment,
                        y=out_sentiment$sentiment)
t_sentiment <- sig_sentiment$statistic %>% as.vector()
p_sentiment <- sig_sentiment$p.value %>% as.vector()
d_sentiment <- compute.es::tes(t_sentiment, 
                               n.1 = n_replies_with_edchat, 
                               n.2 = n_replies_without_edchat, verbose=FALSE)$d %>%
    abs()
row_sentiment <- c(m_in_sentiment, m_out_sentiment, t_sentiment, p_sentiment, d_sentiment)
```

5. Look at the difference in average number of *hashtags* per tweet.

```{r, include=TRUE, echo=FALSE}
m_in_hashtags <- mean(replies_with_edchat$hashtag_count)
m_out_hashtags <- mean(replies_without_edchat$hashtag_count)
sig_hashtags <- t.test(x=replies_with_edchat$hashtag_count,
                       y=replies_without_edchat$hashtag_count)
t_hashtags <- sig_hashtags$statistic %>% as.vector()
p_hashtags <- sig_hashtags$p.value %>% as.vector()
d_hashtags <- compute.es::tes(t_hashtags, 
                               n.1 = n_replies_with_edchat, 
                               n.2 = n_replies_without_edchat, verbose=FALSE)$d %>%
    abs()
row_hashtag <- c(m_in_hashtags, m_out_hashtags, t_hashtags, p_hashtags, d_hashtags)

hashtag_means <- edchat_expanded %>% 
    group_by(org) %>%
    summarize(means = mean(hashtag_count) %>% round(2),
              sds = sd(hashtag_count) %>% round(2),
              vars = var(hashtag_count)
              )
#boxplot(hashtag_count ~ as.factor(org), data = edchat_expanded)

library(car)
leveneTest(hashtag_count ~ as.factor(org), 
                             data = edchat_expanded)

## Kruskal-Wallis non-parametric test to compare means
hashtag_kw <- kruskal.test(hashtag_count ~ as.factor(org), 
                             data = edchat_expanded)
aov(hashtag_count ~ as.factor(org), 
                             data = edchat_expanded)
## I recommend that if you have non-normal data that can't be fixed by transformation, you go ahead and use one-way anova, but be cautious about rejecting the null hypothesis if the P value is not very far below 0.05 and your data are extremely non-normal.

hashtag_row <- hashtag_means %>%
    t() %>%
    as.data.frame() %>%
    rename(both = V1, hashtag = V2, threads = V3) %>%
    mutate(stat = round(hashtag_kw$statistic, 2),
           p = round(hashtag_kw$p.value, 4),
           eta_sq = eta.squared.es(H=as.numeric(hashtag_kw$statistic), 
                                   k=n_groups, 
                                   n=n_obs),
           eps_sq = epsilon.squared.es(H=as.numeric(hashtag_kw$statistic),
                                       n=n_obs)
           ) %>%
    select(hashtag, threads, both, stat, p,  eta_sq, eps_sq)
rownames(hashtag_row) <- "hashtags"
hashtag_row

```

5. Look at the difference in average number of *hyperlinks* per tweet.

```{r, include=TRUE, echo=FALSE}
m_in_hyperlinks <- mean(replies_with_edchat$url_count)
m_out_hyperlinks <- mean(replies_without_edchat$url_count)
sig_hyperlinks <- t.test(x=replies_with_edchat$url_count,
                       y=replies_without_edchat$url_count)
t_hyperlinks <- sig_hyperlinks$statistic %>% as.vector()
p_hyperlinks <- sig_hyperlinks$p.value %>% as.vector()
d_hyperlinks <- compute.es::tes(t_hyperlinks, 
                               n.1 = n_replies_with_edchat, 
                               n.2 = n_replies_without_edchat, verbose=FALSE)$d %>%
    abs()
row_hyperlinks <- c(m_in_hyperlinks, m_out_hyperlinks, t_hyperlinks, p_hyperlinks, d_hyperlinks)
```

6. Look at the difference in average number of *likes* per tweet.

```{r, include=TRUE, echo=FALSE}
m_in_likes <- mean(replies_with_edchat$favorite_count)
m_out_likes <- mean(replies_without_edchat$favorite_count)
sig_likes <- t.test(x=replies_with_edchat$favorite_count,
                    y=replies_without_edchat$favorite_count)
t_likes <- sig_likes$statistic %>% as.vector()
p_likes <- sig_likes$p.value %>% as.vector()
d_likes <- compute.es::tes(t_likes, 
                               n.1 = n_replies_with_edchat, 
                               n.2 = n_replies_without_edchat, verbose=FALSE)$d %>%
    abs()
row_likes <- c(m_in_likes, m_out_likes, t_likes, p_likes, d_likes)
```

7. Look at the difference in average number of *retweets* per tweet.

```{r, include=TRUE, echo=FALSE}
m_in_retweets <- mean(replies_with_edchat$retweet_count)
m_out_retweets <- mean(replies_without_edchat$retweet_count)
sig_retweets <- t.test(x=replies_with_edchat$retweet_count,
                    y=replies_without_edchat$retweet_count)
t_retweets <- sig_retweets$statistic %>% as.vector()
p_retweets <- sig_retweets$p.value %>% as.vector()
d_retweets <- compute.es::tes(t_retweets, 
                               n.1 = n_replies_with_edchat, 
                               n.2 = n_replies_without_edchat, verbose=FALSE)$d %>%
    abs()
row_retweets <- c(m_in_retweets, m_out_retweets, t_retweets, p_retweets, d_retweets)
```

8. Look at the difference in average number of *replies* per tweet.

```{r, include=TRUE, echo=FALSE}
m_in_replies <- mean(replies_with_edchat$reply_count)
m_out_replies <- mean(replies_without_edchat$reply_count)
sig_replies <- t.test(x=replies_with_edchat$reply_count,
                    y=replies_without_edchat$reply_count)
t_replies <- sig_replies$statistic %>% as.vector()
p_replies <- sig_replies$p.value %>% as.vector()
d_replies <- compute.es::tes(t_replies, 
                               n.1 = n_replies_with_edchat, 
                               n.2 = n_replies_without_edchat, verbose=FALSE)$d %>%
    abs()
row_replies <- c(m_in_replies, m_out_replies, t_replies, p_replies, d_replies)
```

```{r, include=TRUE, echo=FALSE}
results_table <- row_word %>%
    rbind(row_character) %>% 
    rbind(row_sentiment) %>%
    rbind(row_hashtag) %>% 
    rbind(row_hyperlinks) %>% 
    rbind(row_likes) %>% 
    rbind(row_retweets) %>% 
    rbind(row_replies) %>%
    as.data.frame() %>%
    round(4)
rownames(results_table) <- c("Words", "Characters", "Sentiment",
                             "Hashtags", "Hyperlinks",
                             "Likes", "Retweets", "Replies")
colnames(results_table) <- c("Mean (in #Edchat)", "Mean (outside #Edchat)",
                             "t", "p", "d")
results_table
#write.csv(results_table, "results_table.csv", row.names=TRUE)
```








# Version/dependencies

```{r, session-info}
sessionInfo()
```

